<h1 id="概述">概述</h1>

<p>产品新增了Elasticsearch功能，在测试ES服务启用过程冲，选择240G 的Intel S4510 型号的SSD一块作为ES的data：</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Model Family:     Intel S4510/S4610/S4500/S4600 Series SSDs
Device Model:     INTEL SSDSC2KG240G8
</code></pre></div></div>

<p>在测试ES启停过程中，发现有个node频繁出现信号被终止:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Jun 19 14:40:07 node76 systemd[1]: Started Session 136 of user root.
Jun 19 14:40:07 node76 systemd[1]: Started Session 137 of user root.
Jun 19 14:40:08 node76 systemd[1]: Started Session 138 of user root.
Jun 19 14:40:08 node76 systemd[1]: Started Session 139 of user root.
Jun 19 14:42:01 node76 CRON[127531]: (root) CMD (bash /usr/local/bin/monitor_ctdb.sh &gt;/dev/null 2&gt;&amp;1)
Jun 19 14:42:09 node76 systemd[1]: Started Session 141 of user root.
Jun 19 14:42:10 node76 systemd[1]: Started Session 142 of user root.
Jun 19 14:42:10 node76 kernel: [ 1764.700814] ata2.00: Enabling discard_zeroes_data
Jun 19 14:42:10 node76 multipath: sdf: using deprecated getuid callout
Jun 19 14:42:11 node76 kernel: [ 1765.708987] ata2.00: Enabling discard_zeroes_data
Jun 19 14:42:11 node76 kernel: [ 1765.713438]  sdf:
Jun 19 14:42:10 node76 multipath: sdf: using deprecated getuid callout
Jun 19 14:42:11 node76 multipath: sdf: using deprecated getuid callout
Jun 19 14:42:11 node76 systemd[1]: Started Session 143 of user root.
Jun 19 14:42:12 node76 kernel: [ 1766.890970] ata2.00: Enabling discard_zeroes_data
Jun 19 14:42:12 node76 kernel: [ 1766.894733] ata2.00: Enabling discard_zeroes_data
Jun 19 14:42:12 node76 kernel: [ 1766.899450] ata2.00: Enabling discard_zeroes_data
Jun 19 14:42:12 node76 multipath: sdf: using deprecated getuid callout
Jun 19 14:42:12 node76 systemd[1]: Started Session 144 of user root.
Jun 19 14:42:12 node76 multipath: sdf: using deprecated getuid callout
Jun 19 14:42:13 node76 systemd[1]: Started Session 145 of user root.
Jun 19 14:42:13 node76 kernel: [ 1767.625828] ata2.00: Enabling discard_zeroes_data
Jun 19 14:42:13 node76 multipath: sdf: using deprecated getuid callout
Jun 19 14:42:13 node76 systemd[1]: Started Session 146 of user root.
Jun 19 14:42:56 node76 smartd[1948]: Device: /dev/bus/6 [megaraid_disk_08] [SAT], SMART Usage Attribute: 194 Temperature_Celsius changed from 171 to 166
Jun 19 14:42:56 node76 smartd[1948]: Device: /dev/bus/6 [megaraid_disk_13] [SAT], SMART Usage Attribute: 194 Temperature_Celsius changed from 142 to 146
Jun 19 14:42:56 node76 smartd[1948]: Device: /dev/bus/6 [megaraid_disk_26] [SAT], SMART Usage Attribute: 194 Temperature_Celsius changed from 166 to 171
Jun 19 14:42:56 node76 smartd[1948]: Device: /dev/bus/6 [megaraid_disk_39] [SAT], SMART Usage Attribute: 194 Temperature_Celsius changed from 171 to 176
Jun 19 14:43:15 node76 multipath: sda: using deprecated getuid callout
Jun 19 14:43:15 node76 multipath: sdb: using deprecated getuid callout
Jun 19 14:43:15 node76 multipath: sdd: using deprecated getuid callout
Jun 19 14:43:15 node76 multipath: sdc: using deprecated getuid callout
Jun 19 14:43:23 node76 radosgw[100762]: 2020-06-19 14:43:23.460381 7f032ec19700 -1 received  signal: Terminated from  PID: 1 task name: /sbin/init  UID: 0
Jun 19 14:43:23 node76 radosgw[100762]: 2020-06-19 14:43:23.460435 7f036e1d2040 -1 shutting down
Jun 19 14:43:23 node76 systemd[1]: Stopping Ceph rados gateway...
Jun 19 14:43:30 node76 systemd[1]: Stopped Ceph rados gateway.
Jun 19 14:44:01 node76 CRON[140607]: (root) CMD (bash /usr/local/bin/monitor_ctdb.sh &gt;/dev/null 2&gt;&amp;1)
Jun 19 14:44:53 node76 systemd[1]: dev-disk-by\x2dpartlabel-datasearch0.device: Job dev-disk-by\x2dpartlabel-datasearch0.device/start timed out.
Jun 19 14:44:53 node76 systemd[1]: Timed out waiting for device INTEL_SSDSC2KG24 datasearch0.
Jun 19 14:44:53 node76 systemd[1]: Dependency failed for /opt/datasearch/0.
Jun 19 14:44:53 node76 systemd[1]: Dependency failed for Local File Systems.
Jun 19 14:44:53 node76 systemd[1]: local-fs.target: Job local-fs.target/start failed with result 'dependency'.
Jun 19 14:44:53 node76 systemd[1]: local-fs.target: Triggering OnFailure= dependencies.
Jun 19 14:44:53 node76 systemd[1]: opt-datasearch-0.mount: Job opt-datasearch-0.mount/start failed with result 'dependency'.
Jun 19 14:44:53 node76 systemd[1]: dev-disk-by\x2dpartlabel-datasearch0.device: Job dev-disk-by\x2dpartlabel-datasearch0.device/start failed with result 'timeout'.
Jun 19 14:44:53 node76 systemd[1]: Stopping ZFS Event Daemon (zed)...
Jun 19 14:44:53 node76 zed[1991]: Exiting
Jun 19 14:44:53 node76 systemd[1]: Stopping Ceph rados gateway...
Jun 19 14:44:53 node76 radosgw[102064]: 2020-06-19 14:44:53.638795 7fc2a48b3700 -1 received  signal: Terminated from  PID: 1 task name: /sbin/init  UID: 0
Jun 19 14:44:53 node76 radosgw[102064]: 2020-06-19 14:44:53.638955 7fc2e3e6c040 -1 shutting down
Jun 19 14:44:53 node76 systemd[1]: Stopping Ceph object storage daemon osd.4...
Jun 19 14:44:53 node76 ceph-osd[6507]: 2020-06-19 14:44:53.639476 7f22760a2700 -1 received  signal: Terminated from  PID: 1 task name: /sbin/init  UID: 0
Jun 19 14:44:53 node76 ceph-osd[6507]: 2020-06-19 14:44:53.639523 7f22760a2700 -1 osd.4 4543 *** Got signal Terminated ***
Jun 19 14:44:53 node76 systemd[1]: Stopped Daily apt upgrade and clean activities.
Jun 19 14:44:53 node76 systemd[1]: Stopping User Manager for UID 0...
Jun 19 14:44:53 node76 systemd[1]: Stopping Getty on tty1...
Jun 19 14:44:53 node76 systemd[1]: Stopped ZFS file system shares.
Jun 19 14:44:53 node76 systemd[1]: Stopped target Multi-User System.
Jun 19 14:44:53 node76 systemd[1]: Stopping Deferred execution scheduler...
Jun 19 14:44:53 node76 systemd[4035]: Reached target Shutdown.
Jun 19 14:44:53 node76 systemd[1]: Stopping ezmonitor service...
Jun 19 14:44:53 node76 systemd[4035]: Starting Exit the Session...
Jun 19 14:44:53 node76 systemd[4035]: Stopped target Default.
Jun 19 14:44:53 node76 systemd[4035]: Stopped target Basic System.
Jun 19 14:44:53 node76 systemd[4035]: Stopped target Sockets.
Jun 19 14:44:53 node76 systemd[4035]: Stopped target Paths.
Jun 19 14:44:53 node76 systemd[4035]: Stopped target Timers.
Jun 19 14:44:53 node76 systemd[1]: Stopping D-Bus System Message Bus...
Jun 19 14:44:53 node76 systemd[1]: Stopping ezrpc service...
Jun 19 14:44:53 node76 systemd[1]: Stopping LSB: OpenIPMI Driver init script...
Jun 19 14:44:53 node76 systemd[1]: Stopping ezs3 webdav service...
Jun 19 14:44:53 node76 systemd[1]: Stopping Self Monitoring and Reporting Technology (SMART) Daemon...
Jun 19 14:44:53 node76 systemd[1]: Stopping OpenBSD Secure Shell server...
Jun 19 14:44:53 node76 smartd[1948]: smartd received signal 15: Terminated
Jun 19 14:44:53 node76 systemd[4035]: Received SIGRTMIN+24 from PID 141392 (kill).
Jun 19 14:44:53 node76 smartd[1948]: Device: /dev/sde [SAT], state written to /var/lib/smartmontools/smartd.INTEL_SSDSC2KG240G8-BTYG8505025C240AGN.ata.state
Jun 19 14:44:53 node76 systemd[1]: Stopping LSB: ha_logd logging daemon...
Jun 19 14:44:53 node76 smartd[1948]: Device: /dev/sdf [SAT], state written to /var/lib/smartmontools/smartd.INTEL_SSDSC2KG240G8-BTYG8505025R240AGN.ata.state
Jun 19 14:44:53 node76 systemd[1]: Stopping Regular background program processing daemon...
Jun 19 14:44:53 node76 smartd[1948]: Device: /dev/bus/6 [megaraid_disk_07] [SAT], state written to /var/lib/smartmontools/smartd.HGST_HUS728T8TALE6L4-VAGWMN2L.ata.state
Jun 19 14:44:53 node76 systemd[1]: Stopping LSB: Set the CPU Frequency Scaling governor to "ondemand"...
Jun 19 14:44:53 node76 smartd[1948]: Device: /dev/bus/6 [megaraid_disk_08] [SAT], state written to /var/lib/smartmontools/smartd.HGST_HUS728T8TALE6L4-VAGWMDPL.ata.state
Jun 19 14:44:53 node76 systemd[1]: Stopping Avahi mDNS/DNS-SD Stack...
Jun 19 14:44:53 node76 smartd[1948]: Device: /dev/bus/6 [megaraid_disk_09] [SAT], state written to /var/lib/smartmontools/smartd.HGST_HUS728T8TALE6L4-VAGWMBHL.ata.state
Jun 19 14:44:53 node76 systemd[1]: Stopping ezs3-app service...
Jun 19 14:44:53 node76 smartd[1948]: Device: /dev/bus/6 [megaraid_disk_10] [SAT], state written to /var/lib/smartmontools/smartd.HGST_HUS728T8TALE6L4-VAGWMK3L.ata.state
Jun 19 14:44:53 node76 systemd[1]: Stopping LSB: Start NTP daemon...
Jun 19 14:44:53 node76 smartd[1948]: Device: /dev/bus/6 [megaraid_disk_11] [SAT], state written to /var/lib/smartmontools/smartd.HGST_HUS728T8TALE6L4-VAGWH8XL.ata.state
Jun 19 14:44:53 node76 systemd[1]: Stopping btmds-agent service...
Jun 19 14:44:53 node76 smartd[1948]: Device: /dev/bus/6 [megaraid_disk_12] [SAT], state written to /var/lib/smartmontools/smartd.HGST_HUS728T8TALE6L4-VAGWMDJL.ata.state
Jun 19 14:44:53 node76 systemd[1]: Stopped Message of the Day.
Jun 19 14:44:53 node76 avahi-daemon[1871]: Got SIGTERM, quitting.
Jun 19 14:44:53 node76 systemd[1]: Stopping LSB: MD monitoring daemon...
Jun 19 14:44:53 node76 smartd[1948]: Device: /dev/bus/6 [megaraid_disk_13] [SAT], state written to /var/lib/smartmontools/smartd.HGST_HUS728T8TALE6L4-VAGMP23L.ata.state
Jun 19 14:44:53 node76 systemd[1]: Stopping LSB: Start or stop stunnel 4.x (SSL tunnel for network daemons)...
Jun 19 14:44:53 node76 avahi-daemon[1871]: Leaving mDNS multicast group on interface bond1.IPv4 with address 10.10.10.76.
Jun 19 14:44:53 node76 systemd[1]: Closed RPCbind Server Activation Socket.
Jun 19 14:44:53 node76 avahi-daemon[1871]: Leaving mDNS multicast group on interface bond0.IPv4 with address 10.6.7.76.
Jun 19 14:44:53 node76 systemd[1]: Stopping LSB: Start/stop sysstat's sadc...
Jun 19 14:44:53 node76 avahi-daemon[1871]: Leaving mDNS multicast group on interface enp61s0f0.IPv4 with address 172.17.73.76.
Jun 19 14:44:53 node76 systemd[1]: Stopping LSB: Mount debugfs on /sys/kernel/debug...
Jun 19 14:44:53 node76 smartd[1948]: Device: /dev/bus/6 [megaraid_disk_14] [SAT], state written to /var/lib/smartmontools/smartd.HGST_HUS728T8TALE6L4-VAGW040L.ata.state
Jun 19 14:44:53 node76 systemd[1]: Stopping ACPI event daemon...
Jun 19 14:44:53 node76 ceph-osd[6513]: 2020-06-19 14:44:53.659295 7f6dcb1bf700 -1 received  signal: Terminated from  PID: 1 task name: /sbin/init  UID: 0
Jun 19 14:44:53 node76 ceph-osd[6513]: 2020-06-19 14:44:53.659347 7f6dcb1bf700 -1 osd.6 4543 *** Got signal Terminated ***
Jun 19 14:44:53 node76 smartd[1948]: Device: /dev/bus/6 [megaraid_disk_15] [SAT], state written to /var/lib/smartmontools/smartd.HGST_HUS728T8TALE6L4-VAGR58DL.ata.state
Jun 19 14:44:53 node76 systemd[1]: Stopping Ceph object storage daemon osd.6...
Jun 19 14:44:53 node76 ceph-mds[106432]: 2020-06-19 14:44:53.660418 7f7f1374e700 -1 received  signal: Terminated from  PID: 1 task name: /sbin/init  UID: 0
Jun 19 14:44:53 node76 ceph-mds[106432]: 2020-06-19 14:44:53.660474 7f7f1374e700 -1 mds.jxrzm *** got signal Terminated ***
Jun 19 14:44:53 node76 smartd[1948]: Device: /dev/bus/6 [megaraid_disk_16] [SAT], state written to /var/lib/smartmontools/smartd.HGST_HUS728T8TALE6L4-VAGV89JL.ata.state
Jun 19 14:44:53 node76 systemd[1]: Stopped Daily Cleanup of Temporary Directories.
Jun 19 14:44:53 node76 ceph-osd[6515]: 2020-06-19 14:44:53.661958 7fec27b33700 -1 received  signal: Terminated from  PID: 1 task name: /sbin/init  UID: 0
Jun 19 14:44:53 node76 ceph-osd[6515]: 2020-06-19 14:44:53.661992 7fec27b33700 -1 osd.5 4543 *** Got signal Terminated ***
Jun 19 14:44:53 node76 smartd[1948]: Device: /dev/bus/6 [megaraid_disk_17] [SAT], state written to /var/lib/smartmontools/smartd.HGST_HUS728T8TALE6L4-VAGW04VL.ata.state
Jun 19 14:44:53 node76 systemd[1]: Stopping LSB: ipvsadm daemon...
Jun 19 14:44:53 node76 avahi-daemon[1871]: avahi-daemon 0.6.32-rc exiting.
Jun 19 14:44:53 node76 systemd[1]: Stopping Ceph metadata server daemon...
Jun 19 14:44:53 node76 smartd[1948]: Device: /dev/bus/6 [megaraid_disk_19] [SAT], state written to /var/lib/smartmontools/smartd.HGST_HUS728T8TALE6L4-VAGUUYSL.ata.state
Jun 19 14:44:53 node76 systemd[1]: Stopped target Mail Transport Agent.
Jun 19 14:44:53 node76 smartd[1948]: Device: /dev/bus/6 [megaraid_disk_21] [SAT], state written to /var/lib/smartmontools/smartd.HGST_HUS728T8TALE6L4-VAGW0B1L.ata.state
Jun 19 14:44:53 node76 systemd[1]: Stopping LSB: Starts the Name Service Cache Daemon...
Jun 19 14:44:53 node76 smartd[1948]: Device: /dev/bus/6 [megaraid_disk_22] [SAT], state written to /var/lib/smartmontools/smartd.HGST_HUS728T8TALE6L4-VAGLKRHL.ata.state
Jun 19 14:44:53 node76 systemd[1]: Stopping Ceph object storage daemon osd.5...
Jun 19 14:44:53 node76 smartd[1948]: Device: /dev/bus/6 [megaraid_disk_23] [SAT], state written to /var/lib/smartmontools/smartd.HGST_HUS728T8TALE6L4-VAGV8RRL.ata.state
Jun 19 14:44:53 node76 systemd[1]: Stopping Map RBD devices...
Jun 19 14:44:53 node76 smartd[1948]: Device: /dev/bus/6 [megaraid_disk_25] [SAT], state written to /var/lib/smartmontools/smartd.HGST_HUS728T8TALE6L4-VAGVJLAL.ata.state
Jun 19 14:44:53 node76 smartd[1948]: Device: /dev/bus/6 [megaraid_disk_26] [SAT], state written to /var/lib/smartmontools/smartd.HGST_HUS728T8TALE6L4-VAGV8JPL.ata.state
Jun 19 14:44:53 node76 smartd[1948]: Device: /dev/bus/6 [megaraid_disk_27] [SAT], state written to /var/lib/smartmontools/smartd.HGST_HUS728T8TALE6L4-VAGV8WRL.ata.state
Jun 19 14:44:53 node76 smartd[1948]: Device: /dev/bus/6 [megaraid_disk_28] [SAT], state written to /var/lib/smartmontools/smartd.HGST_HUS728T8TALE6L4-VAGV88VL.ata.state
Jun 19 14:44:53 node76 smartd[1948]: Device: /dev/bus/6 [megaraid_disk_29] [SAT], state written to /var/lib/smartmontools/smartd.HGST_HUS728T8TALE6L4-VAGVVYLL.ata.state
Jun 19 14:44:53 node76 smartd[1948]: Device: /dev/bus/6 [megaraid_disk_31] [SAT], state written to /var/lib/smartmontools/smartd.HGST_HUS728T8TALE6L4-VAGDRZNL.ata.state
Jun 19 14:44:53 node76 smartd[1948]: Device: /dev/bus/6 [megaraid_disk_34] [SAT], state written to /var/lib/smartmontools/smartd.HGST_HUS728T8TALE6L4-VAGWK13L.ata.state
Jun 19 14:44:53 node76 smartd[1948]: Device: /dev/bus/6 [megaraid_disk_35] [SAT], state written to /var/lib/smartmontools/smartd.TOSHIBA_MG05ACA800E-29QIK7N4FUUD.ata.state
Jun 19 14:44:53 node76 smartd[1948]: Device: /dev/bus/6 [megaraid_disk_37] [SAT], state written to /var/lib/smartmontools/smartd.HGST_HUS728T8TALE6L4-VAGW1L0L.ata.state
Jun 19 14:44:53 node76 smartd[1948]: Device: /dev/bus/6 [megaraid_disk_38] [SAT], state written to /var/lib/smartmontools/smartd.HGST_HUS728T8TALE6L4-VAGW3A9L.ata.state
Jun 19 14:44:53 node76 smartd[1948]: Device: /dev/bus/6 [megaraid_disk_39] [SAT], state written to /var/lib/smartmontools/smartd.HGST_HUS728T8TALE6L4-VAG88E1D.ata.state
Jun 19 14:44:53 node76 smartd[1948]: Device: /dev/bus/6 [megaraid_disk_40] [SAT], state written to /var/lib/smartmontools/smartd.HGST_HUS728T8TALE6L4-VAGW7ATL.ata.state
Jun 19 14:44:53 node76 smartd[1948]: Device: /dev/bus/6 [megaraid_disk_41] [SAT], state written to /var/lib/smartmontools/smartd.HGST_HUS728T8TALE6L4-VAGWG8AL.ata.state
Jun 19 14:44:53 node76 smartd[1948]: Device: /dev/bus/6 [megaraid_disk_42] [SAT], state written to /var/lib/smartmontools/smartd.HGST_HUS728T8TALE6L4-VAGW0BWL.ata.state
Jun 19 14:44:53 node76 smartd[1948]: Device: /dev/nvme0, state written to /var/lib/smartmontools/smartd.INTEL_SSDPEDKX040T7-PHLF841500HK4P0DGN.nvme.state
Jun 19 14:44:53 node76 smartd[1948]: smartd is exiting (exit status 0)
Jun 19 14:44:53 node76 systemd[1]: Stopping ezs3 bucket logging agent service...
Jun 19 14:44:53 node76 systemd[1]: Stopping ezs3 agent service...
Jun 19 14:44:53 node76 systemd[1]: Stopping Ceph cluster manager daemon...
Jun 19 14:44:53 node76 ceph-mgr[2607]: 2020-06-19 14:44:53.669992 7f936327b700 -1 received  signal: Terminated from  PID: 1 task name: /sbin/init  UID: 0
Jun 19 14:44:53 node76 ceph-mgr[2607]: 2020-06-19 14:44:53.670108 7f936327b700 -1 mgr handle_signal *** Got signal Terminated ***
Jun 19 14:44:53 node76 systemd[1]: Stopping csmonitor service...
Jun 19 14:44:53 node76 systemd[1]: Stopping LSB: CacheFiles daemon...
Jun 19 14:44:53 node76 systemd[1]: Stopping LSB: Postfix Mail Transport Agent...
Jun 19 14:44:53 node76 rsyslogd: [origin software="rsyslogd" swVersion="8.16.0" x-pid="27997" x-info="http://www.rsyslog.com"] exiting on signal 15.
Jun 19 14:51:31 node76 rsyslogd: [origin software="rsyslogd" swVersion="8.16.0" x-pid="1973" x-info="http://www.rsyslog.com"] start
Jun 19 14:51:31 node76 systemd-modules-load[662]: Inserted module 'iscsi_tcp'
Jun 19 14:51:31 node76 systemd-modules-load[662]: Inserted module 'ib_iser'
Jun 19 14:51:31 node76 rsyslogd-2222: command 'KLogPermitNonKernelFacility' is currently not permitted - did you already set it via a RainerScript command (v6+ config)? [v8.16.0 try http://www.rsyslog.com/e/2222 ]
Jun 19 14:51:31 node76 rsyslogd-2307: warning: ~ action is deprecated, consider using the 'stop' statement instead [v8.16.0 try http://www.rsyslog.com/e/2307 ]
Jun 19 14:51:31 node76 loadkeys[664]: Loading /etc/console-setup/cached.kmap.gz
Jun 19 14:51:31 node76 ureadahead[699]: ureadahead: Error while tracing: No such file or directory
Jun 19 14:51:31 node76 rsyslogd-2307: message repeated 3 times: [warning: ~ action is deprecated, consider using the 'stop' statement instead [v8.16.0 try http://www.rsyslog.com/e/2307 ]]
Jun 19 14:51:31 node76 rsyslogd: rsyslogd's groupid changed to 108
Jun 19 14:51:31 node76 rsyslogd: rsyslogd's userid changed to 104
Jun 19 14:51:31 node76 systemd[1]: Started Load Kernel Modules.
Jun 19 14:51:31 node76 systemd[1]: Started Set console keymap.
Jun 19 14:51:31 node76 systemd[1]: Started Remount Root and Kernel File Systems.
Jun 19 14:51:31 node76 multipathd[707]: sde: using deprecated getuid callout
Jun 19 14:51:31 node76 multipathd[707]: sdf: using deprecated getuid callout
Jun 19 14:51:31 node76 multipathd[707]: sda: using deprecated getuid callout
Jun 19 14:51:31 node76 multipathd[707]: sdb: using deprecated getuid callout
Jun 19 14:51:31 node76 multipathd[707]: sdc: using deprecated getuid callout
Jun 19 14:51:31 node76 multipathd[707]: sdd: using deprecated getuid callout
Jun 19 14:51:31 node76 systemd[1]: ureadahead.service: Main process exited, code=exited, status=5/NOTINSTALLED
Jun 19 14:51:31 node76 systemd[1]: ureadahead.service: Unit entered failed state.
Jun 19 14:51:31 node76 systemd[1]: ureadahead.service: Failed with result 'exit-code'.
Jun 19 14:51:31 node76 systemd[1]: Started Create list of required static device nodes for the current kernel.
Jun 19 14:51:31 node76 systemd[1]: Started Nameserver information manager.
Jun 19 14:51:31 node76 kernel: [    0.000000] Linux version 4.14.148-server (wsg@NjBUild8) (gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.12)) #1 SMP Thu Jun 11 18:49:19 CST 2020
</code></pre></div></div>

<p>说明一下：
    上文中的sdf这个分区，是启用ES时选择的ES DATA disk,为了能够复现这个问题，写了支script指定循环次数去启用，停用ES 服务，并检查对应的挂载点，ES集群健康状态，ceph集群健康状态（因为出现shutdown ceph核心服务进行，ceph集群不健康作为退出条件）.</p>

<h1 id="python脚本如下">python脚本如下：</h1>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>root@node76:~# cat enable_disable_es.py 
#!/usr/bin/env python
# -*- coding:UTF-8 -*-

import os
import sys
import time
import json
import requests
from requests.exceptions import ConnectionError
from requests.packages.urllib3.exceptions import InsecureRequestWarning
requests.packages.urllib3.disable_warnings(InsecureRequestWarning)


def login_session(public_ip, user_id, password):
    login_url = "https://{}:8080/auth".format(public_ip)
    login_data = {"user_id": user_id, "password": password}

    session = requests.session()
    session.keep_alive = False
    session.headers.update({'Accept': 'application/json, text/javascript, */*; q=0.01'})
    response = session.request("POST", login_url, json=login_data, verify=False)

    if response.status_code != 200:
        print("[ERROR]    Login web error, http status code is {} \n".format(response.status_code))
        sys.exit(1)

    return session


def http_request(session, http_method, url, params=None, data=None, public_ip=None, user_id=None, password=None):
    params = {} if params is None else params
    data = {} if data is None else data

    # set SSL Verify to False
    session.verify = False

    # Set session keep alive to False
    session.keep_alive = False

    if "cgi-bin" in url:
        # Legacy CGI
        try:
            response = session.request(http_method, url, params=params, data=data)
        except ConnectionError as ex:
            time.sleep(2)
            response = session.request(http_method, url, params=params, data=data)

        if response.status_code == 401:
            session = login_session(public_ip, user_id, password)
            response = session.request(http_method, url, params=params, data=data)
    else:
        # Restful CGI
        response = session.request(http_method, url, params=params, json=data)

        if response.status_code == 401:
            session = login_session(public_ip, user_id, password)
            response = session.request(http_method, url, params=params, json=data)

    if response.status_code == 500:
        print("[ERROR]    Send HTTP request failed, backend return 500, internal server error \n")
        sys.exit(1)

    return response


def es_enable_data(public_ip, storage_ip, vip):
    data = {
        "ip":storage_ip,
        "vip":vip,
        "router_id":"80",
        "num_shards":"6",
        "num_replicas":"1",
        "storage_location":"disk",
        "disks":"[\"/dev/sdf\"]"
    }

    return data


def enable_es(public_ip, storage_ip, session, http_method, url, data):
    cur_time = time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time()))
    print("\n[{}] [Action]   Start to enable ES service on node : ({})".format(cur_time, storage_ip))

    response = http_request(session, http_method, url, data=data)

    if json.loads(response.text)['return_code'] == 0:
        progress_url = "https://{}:8080/cgi-bin/ezs3/json/elasticsearch_role_progress".format(public_ip)
        check_progress(session, storage_ip, progress_url)

        check_fstab(storage_ip)
        check_mount_point(storage_ip)
    else:
        print("[ERROR]    Enable ES service failed, backend return : ({})".format(response.text))
        sys.exit(1)


def es_disable_data(storage_ip):
    data = {'ip': storage_ip}

    return data

def disable_es(public_ip, storage_ip, session, http_method, url, data):
    cur_time = time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time()))
    print("\n[{}] [Action]   Disable ES service on node : ({})".format(cur_time, storage_ip))

    response = http_request(session, http_method, url, data=data)

    if json.loads(response.text)['return_code'] == 0:
        progress_url = "https://{}:8080/cgi-bin/ezs3/json/elasticsearch_role_progress".format(public_ip)
        check_progress(session, storage_ip, progress_url, enable_flag=False)

        check_fstab(storage_ip, enable_flag=False)
        check_mount_point(storage_ip, enable_flag=False)
    else:
        print("[ERROR]    Disable ES service failed, backend return : ({})".format(response.text))
        sys.exit(1)


def progress_params(storae_ip, enable='true'):
    param = {'ip': storae_ip, 'enable': enable}

    return param


def get_enable_progress(session, url, param):
    for i in xrange(60):
        response = http_request(session, "GET", url, params=param) 
        res = json.loads(response.text)
        if res['return_code'] == 0:
            progress = res['response']['info']['progress']
            if progress == 100:
                break
            else:
                if i == 30:
                    print("                          Wait for more than 150s, go on")
                if i &gt;= 58:
                    print("                          ES progress is : ({})".format(progress))
                time.sleep(5)
    else:
        cur_time = time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time()))
        print("[{}] [ERROR]    Check ES progress failed, exit! \n".format(cur_time))
        sys.exit(1)


def check_progress(session, storage_ip, url, enable_flag=True):
    cur_time = time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time()))
    if enable_flag:
        print("[{}] [Check]    Check ES enable progress on node : ({})".format(cur_time, storage_ip))
        param = progress_params(storage_ip)
    else:
        print("[{}] [Check]    Check ES disable progress on node : ({})".format(cur_time, storage_ip))
        param = progress_params(storage_ip, enable='false')

    get_enable_progress(session, url, param=param)


def get_radosgw_pid(hosts):
    host_rgw_pid = {}
    for each_host in hosts:
        pid = os.popen("ssh {} ps aux | grep client.radosgw.0 | grep -v grep | awk ''").read().strip()
        if pid:
            host_rgw_pid[each_host] = pid
        else:
            cur_time = time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time()))
            print("[{}] [ERROR]    Not get radosgw pid on node :({})".foramt(cur_time, each_host))
            sys.exit(1)

    return host_rgw_pid


def check_fstab(storage_ip, enable_flag=True):
    cur_time = time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time()))
    print("[{}] [Check]    Check mount config in /etc/fstab on node : ({})".format(cur_time, storage_ip))

    fstab_content = os.popen("ssh {} 'cat /etc/fstab | grep datasearch'".format(storage_ip)).read().strip()
    if enable_flag:
        if "datasearch" not in fstab_content:
            cur_time = time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time()))
            print("[{}] [ERROR]    Enable ES, but not find mount point information in /etc/fstab \n".format(cur_time))
            sys.exit(1)
    else:
        if "datasearch" in fstab_content:
            cur_time = time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time()))
            print("[{}] [ERROR]    Disable ES, but still find mount point information in /etc/fstab \n".format(cur_time))
            sys.exit(1)


def check_mount_point(storage_ip, enable_flag=True, pre_check=False):
    cur_time = time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time()))
    if not pre_check:
        print("[{}] [Check]    Check mount point on node : ({})".format(cur_time, storage_ip))

    mount_info = os.popen("ssh {} 'mount | grep datasearch'".format(storage_ip)).read().strip()
    if enable_flag:
        if not mount_info:
            cur_time = time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time()))
            print("[{}] [ERROR]    Enable ES, but not find mount point \n".format(cur_time))
            sys.exit(1)
    else:
        if mount_info:
            cur_time = time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time()))
            if not pre_check:
                print("[{}] [ERROR]    Disable ES, but still find mount point \n".format(cur_time))
            else:
                print("[{}] [ERROR]    Precheck before enable ES, but still find mount point on node : ({}) \n".format(cur_time, storage_ip))
            sys.exit(1)


def check_es_cluster_helath(public_ip):
    cur_time = time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time()))
    print("\n[{}] [Check]    Check ES cluster status".format(cur_time))

    cmd = "curl -s GET http://{}:9200/_cluster/health | json_pp | grep status".format(public_ip)
    for i in xrange(30):
        es_health = os.popen(cmd).read().strip()
        if 'green' not in es_health:
            time.sleep(5)
            if i == 29:
                cur_time = time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time()))
                print("[{}] [ERROR]    ES Cluser helath status is : ({})".format(cur_time, es_health))
        else:
            break
    else:
        cur_time = time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time()))
        print("[{}] [ERROR]  Wait 60s, but ES cluser status is not 'green' \n".format(cur_time))
        sys.exit(1)


def check_ceph_cluster_health():
    cur_time = time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time()))
    print("\n[{}] [Check]    Check ceph cluster status".format(cur_time))

    cmd = "ceph -s | grep health"
    for i in xrange(30):
        ceph_health = os.popen(cmd).read().strip()
        if "HEALTH_OK" not in ceph_health:
            time.sleep(5)
        else:
            break
    else:
        cur_time = time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time()))
        print("[{}] [ERROR]  Wait 60s, but ceoh cluser status is not 'HEALTH_OK' \n".format(cur_time))
        sys.exit(1)


def loop_run(loop_times):
    password = "1"
    user_id = "admin"
    vip = "10.6.7.80/22"
    public_ip = "10.6.7.75"
    storage_ips = ["10.10.10.75", "10.10.10.76", "10.10.10.77"]

    session = login_session(public_ip, user_id, password)
    enable_es_url = "https://{}:8080/cgi-bin/ezs3/json/elasticsearch_role_enable".format(public_ip)
    disable_es_url = "https://{}:8080/cgi-bin/ezs3/json/elasticsearch_role_disable".format(public_ip)

    for i in xrange(loop_times):
        print("\n---------------------------------------------- {} --------------------------------------------".format(i+1))
        # Before disable, check datasearch mount point again, because not known why the mount point mount again
        cur_time = time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time()))
        print("[{}] [Check]    Precheck mount point on each node before enable ES service".format(cur_time))

        # Only check once by the first time
        if i &lt; 1:
            for each_storage_ip in storage_ips:
                check_mount_point(each_storage_ip, enable_flag=False, pre_check=True)
            time.sleep(1)
        else:
            for j in xrange(65):
                for each_storage_ip in storage_ips:
                    check_mount_point(each_storage_ip, enable_flag=False, pre_check=True)
                time.sleep(1)

        # Enable ES service on all node
        for each_storage_ip in storage_ips:
            enable_es_data = es_enable_data(public_ip, each_storage_ip, vip)
            enable_es(public_ip, each_storage_ip, session, "POST", enable_es_url, data=enable_es_data)

        # Check ES cluser status
        os.popen("iptables -F")
        check_es_cluster_helath(public_ip)

        # Disable ES service on all node
        for each_storage_ip in storage_ips:
            disable_es_data = es_disable_data(each_storage_ip) 
            disable_es(public_ip, each_storage_ip, session, "POST", disable_es_url, data=disable_es_data)


        # Check ceph cluster status
        check_ceph_cluster_health()
        time.sleep(2)


if __name__ == '__main__':
    loop_times = sys.argv[1]

    if loop_times &lt; 1:
        print("[ERROR]  loop_times must &gt;=1 \n")
        sys.exit(1)

    loop_run(int(loop_times))
</code></pre></div></div>

<h1 id="执行效果">执行效果</h1>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>root@node75:~# python enable_disable_es.py 3

---------------------------------------------- 1 --------------------------------------------
[Action]   Start to force to clean umount and disk on node : (10.10.10.75)
umount: /opt/datasearch/0: mountpoint not found
[Action]   Start to force to clean umount and disk on node : (10.10.10.76)
umount: /opt/datasearch/0: mountpoint not found
[Action]   Start to force to clean umount and disk on node : (10.10.10.77)
umount: /opt/datasearch/0: mountpoint not found

[2020-06-19 14:17:39] [Action]   Start to enable ES service on node : (10.10.10.75)
[2020-06-19 14:17:40] [Check]    Check ES enable progress on node : (10.10.10.75)
[2020-06-19 14:19:41] [Check]    Check mount config in /etc/fstab on node : (10.10.10.75)
[2020-06-19 14:19:41] [Check]    Check mount point on node : (10.10.10.75)

[2020-06-19 14:19:41] [Action]   Start to enable ES service on node : (10.10.10.76)
[2020-06-19 14:19:41] [Check]    Check ES enable progress on node : (10.10.10.76)
[2020-06-19 14:20:22] [Check]    Check mount config in /etc/fstab on node : (10.10.10.76)
[2020-06-19 14:20:22] [Check]    Check mount point on node : (10.10.10.76)

[2020-06-19 14:20:22] [Action]   Start to enable ES service on node : (10.10.10.77)
[2020-06-19 14:20:22] [Check]    Check ES enable progress on node : (10.10.10.77)
[2020-06-19 14:20:58] [Check]    Check mount config in /etc/fstab on node : (10.10.10.77)
[2020-06-19 14:20:58] [Check]    Check mount point on node : (10.10.10.77)

[2020-06-19 14:20:58] [Check]    Check ES cluster status

[2020-06-19 14:21:04] [Action]   Disable ES service on node : (10.10.10.75)
[2020-06-19 14:21:04] [Check]    Check ES disable progress on node : (10.10.10.75)
[2020-06-19 14:21:14] [Check]    Check mount config in /etc/fstab on node : (10.10.10.75)
[2020-06-19 14:21:14] [Check]    Check mount point on node : (10.10.10.75)

[2020-06-19 14:21:14] [Action]   Disable ES service on node : (10.10.10.76)
[2020-06-19 14:21:14] [Check]    Check ES disable progress on node : (10.10.10.76)
[2020-06-19 14:21:24] [Check]    Check mount config in /etc/fstab on node : (10.10.10.76)
[2020-06-19 14:21:25] [Check]    Check mount point on node : (10.10.10.76)

[2020-06-19 14:21:25] [Action]   Disable ES service on node : (10.10.10.77)
[2020-06-19 14:21:25] [Check]    Check ES disable progress on node : (10.10.10.77)
[2020-06-19 14:21:50] [Check]    Check mount config in /etc/fstab on node : (10.10.10.77)
[2020-06-19 14:21:50] [Check]    Check mount point on node : (10.10.10.77)

[2020-06-19 14:21:51] [Check]    Check ceph cluster status
[Action]   Start to force to clean umount and disk on node : (10.10.10.75)
umount: /opt/datasearch/0: mountpoint not found
[Action]   Start to force to clean umount and disk on node : (10.10.10.76)
umount: /opt/datasearch/0: mountpoint not found
[Action]   Start to force to clean umount and disk on node : (10.10.10.77)
umount: /opt/datasearch/0: mountpoint not found

---------------------------------------------- 2 --------------------------------------------
[Action]   Start to force to clean umount and disk on node : (10.10.10.75)
umount: /opt/datasearch/0: mountpoint not found
[Action]   Start to force to clean umount and disk on node : (10.10.10.76)
umount: /opt/datasearch/0: mountpoint not found
[Action]   Start to force to clean umount and disk on node : (10.10.10.77)
umount: /opt/datasearch/0: mountpoint not found

[2020-06-19 14:22:13] [Action]   Start to enable ES service on node : (10.10.10.75)
[2020-06-19 14:22:14] [Check]    Check ES enable progress on node : (10.10.10.75)
                                 Wait for more than 150s, go on
                                 ES progress is : (30)
                                 ES progress is : (30)
[2020-06-19 14:27:16] [ERROR]    Check ES progress failed, exit! 

root@node75:~#
</code></pre></div></div>

